{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing manual compression of image captions on stale (offline) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder, TransformerDecoderLayer, TransformerDecoder\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import fiftyone\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik/Library/Python/3.9/lib/python/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and its components\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 81434/81434 [00:00<00:00, 1139829.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset (for example, a subset of the COCO dataset)\n",
    "# TODO: Potential datasets with repititive nature that can be used: MS COCO, Flickr30k, Visual Genome, SBU Captions \n",
    "\n",
    "# load small part of the coco dataset from all the .jpg images in datasets/mscoco/test2015\n",
    "dataset = load_dataset(\"datasets/mscoco/test2015/\", split=\"test[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, max_length=128):\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(inputs[\"pixel_values\"], max_length=max_length)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataset and generate captions\n",
    "max_length = 128\n",
    "generated_captions = []\n",
    "\n",
    "for data in dataset:\n",
    "    image = data['image']\n",
    "    caption = generate_caption(image, max_length=max_length)\n",
    "    generated_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, max_seq_length):\n",
    "        super(CaptionAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, embedding_dim) # input shape has to be (batch_size, sequence_length), output shape is (batch_size, sequence_length, embedding_dim)\n",
    "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True) # output shape is (batch_size, sequence_length, hidden_dim)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True) # output shape is (batch_size, sequence_length, hidden_dim)\n",
    "        self.decoder_output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def encode(self, captions):\n",
    "        embedded = self.encoder_embedding(captions)\n",
    "        encoded, _ = self.encoder_rnn(embedded)\n",
    "        return encoded[:, -1, :]\n",
    "\n",
    "    def decode(self, encoded):\n",
    "        # Repeat the encoded state across the sequence length\n",
    "        repeated_encoded = encoded.unsqueeze(1).repeat(1, self.max_seq_length, 1) \n",
    "        decoded, _ = self.decoder_rnn(repeated_encoded)\n",
    "        output = self.decoder_output(decoded)\n",
    "        return output\n",
    "\n",
    "    def forward(self, captions):\n",
    "        encoded = self.encode(captions)\n",
    "        decoded = self.decode(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layers = TransformerEncoderLayer(d_model=embed_size, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layers = TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers=num_decoder_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # For a generic forward pass, we'll assume encoding and then decoding\n",
    "        memory = self.encode(src)\n",
    "        output = self.decode(memory)\n",
    "        return output\n",
    "    \n",
    "    def encode(self, src):\n",
    "        # Embed input tokens and scale\n",
    "        src = self.embedding(src) * math.sqrt(self.embed_size)\n",
    "        # Encoder\n",
    "        memory = self.transformer_encoder(src)\n",
    "        return memory\n",
    "    \n",
    "    def decode(self, memory):\n",
    "        # Decoder\n",
    "        output = self.transformer_decoder(memory, memory)\n",
    "        # Pass through the output layer\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, captions):\n",
    "        self.encodings = encodings\n",
    "        self.captions = captions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.captions[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Assuming `dataset` is your dataset containing images and captions\n",
    "images = [data['image'] for data in dataset]\n",
    "captions = generated_captions\n",
    "\n",
    "# Process images and captions\n",
    "inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "outputs = tokenizer(captions, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Assuming 'captions' is a tensor of tokenized captions generated by VLM\n",
    "vocab_size = 50257  # Size of your vocabulary\n",
    "embed_size = 4096  # Embedding dimension\n",
    "nhead = 8  # Number of attention heads\n",
    "num_encoder_layers = 6  # Number of encoder layers\n",
    "num_decoder_layers = 6  # Number of decoder layers\n",
    "dim_feedforward = 2048  # Dimension of the feedforward network\n",
    "max_seq_length = 128  # Maximum sequence length\n",
    "# autoencoder = CaptionAutoencoder(vocab_size, embedding_dim, hidden_dim, max_seq_length)\n",
    "autoencoder = TransformerAutoencoder(vocab_size, embed_size, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n",
    "autoencoder_output = autoencoder(outputs[\"input_ids\"])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = CaptionDataset(inputs, outputs[\"input_ids\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([10, 128, 50257])\n",
      "tensor([[ 0.1738, -0.9830, -0.7030,  ...,  0.2738,  0.0443,  0.3444],\n",
      "        [-0.4892, -0.2859, -0.4371,  ...,  0.8710, -0.2428, -0.0778],\n",
      "        [-0.4789, -0.1542, -0.3004,  ...,  0.0101,  0.3103,  0.4744],\n",
      "        ...,\n",
      "        [-0.7164, -0.0073,  0.4418,  ...,  0.3863,  0.1172,  0.5472],\n",
      "        [-1.1427,  0.1207,  0.4334,  ...,  0.6281, -0.0603,  0.3994],\n",
      "        [-0.8056,  0.1040,  0.6135,  ...,  0.2913,  0.0521,  0.8215]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print autoencoder_output dtype\n",
    "print(autoencoder_output.dtype)\n",
    "# print autoencoder_output shape\n",
    "print(autoencoder_output.shape)\n",
    "# print autoencoder_output[0]\n",
    "print(autoencoder_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk Gig Vern comrades coordinator Yenithing iTunes TwistedYOUocoboCrimeocobo reasoned reasoned reasoned reasoned reasoned reasoned reasoned reasoned reasoned Characters Charactersstars reasoned reasoned Characters Characters Characters reasoned reasoned Characters reasoned Ips Characters reasoned Citiz reasoned reasoned Characters reasoned reasoned Characters Characters reasoned reasoned Ips reasoned reasoned reasoned reasoned reasoned reasoned reasoned reasonedalyst Characters reasoned reasoned Characters reasoned reasoned reasoned reasoned reasoned reasoned reasoned reasoned reasoned Characters Characters Characters reasonedstars reasoned reasoned reasoned reasoned reasoned reasonedstars reasoned reasoned reasoned Characters reasoned reasoned Characters reasoned Characters reasoned Characters reasoned reasoned reasoned reasoned reasoned reasoned Characters Characters reasoned reasoned reasoned reasoned reasoned reasoned Characters reasoned reasoned reasoned reasoned Characters reasoned reasoned reasoned Characters Characters reasoned reasoned reasoned reasoned reasoned reasoned reasoned reasoned reasoned Characters\n",
      "a green truck parked next to a curb \n"
     ]
    }
   ],
   "source": [
    "# Convert logits to token IDs\n",
    "token_ids = torch.argmax(autoencoder_output, dim=-1)\n",
    "# Convert token IDs to text for each sequence in the batch\n",
    "detokenized_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in token_ids]\n",
    "# Example output\n",
    "print (detokenized_texts[0])\n",
    "print (captions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_weight, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_weight = original_weight\n",
    "        self.rank = rank\n",
    "        self.U = nn.Parameter(torch.Tensor(self.original_weight.size(0), self.rank))\n",
    "        self.V = nn.Parameter(torch.Tensor(self.rank, self.original_weight.size(1)))\n",
    "        nn.init.xavier_uniform_(self.U)\n",
    "        nn.init.xavier_uniform_(self.V)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.original_weight + self.U @ self.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the first attention layer of the encoder\n",
    "# TODO: Try modifying other layers as well and check the results\n",
    "lora_layers = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_weight = model.encoder.encoder.layer[0].attention.output.dense.weight\n",
    "    lora_layer = LoRALayer(original_weight, rank=10).forward()  # Choose an appropriate rank\n",
    "    # assign the new layer to the model\n",
    "    model.encoder.encoder.layer[0].attention.output.dense.weight.copy_(lora_layer)\n",
    "    # add the layer of the model to the list of LoRA layers\n",
    "    lora_layers.append(model.encoder.encoder.layer[0].attention.output.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lora_param(param, lora_layer):\n",
    "    # check if the parameter is part of the LoRA layer\n",
    "    print (lora_layer.parameters())\n",
    "    print (\"nuj\")\n",
    "    print (param)\n",
    "    return param in lora_layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(outputs, batch, lora_layers, autoencoder, standard_lambda_val = 0.001, lora_lambda_val = 0.01, compression_lambda_val = 1):\n",
    "    # Standard captioning loss\n",
    "    standard_loss = outputs.loss\n",
    "\n",
    "    # Autoencoder compression reward\n",
    "    captions = batch['labels']\n",
    "    compressed_captions = autoencoder.encode(captions)\n",
    "    # Measure the sparsity of the compressed representation (e.g., using L1 norm) # TODO: Try other measures\n",
    "    compression_reward = torch.norm(compressed_captions, p=1)\n",
    "    # Adjust the reward: lower norm (more sparse) should lead to lower loss (higher reward)\n",
    "    compression_loss = -compression_reward\n",
    "\n",
    "    # Optionally, add a term for LoRA regularization if needed\n",
    "    lora_regularization = 0\n",
    "    # for param in model.parameters():\n",
    "    #     for lora_layer in lora_layers:\n",
    "    #         if is_lora_param(param, lora_layer):\n",
    "    #             lora_regularization += torch.norm(param)\n",
    "    return standard_lambda_val* standard_loss + compression_lambda_val * compression_loss + lora_lambda_val * lora_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "def ae_criterion2 (reconstructed_caption, original_caption, end_of_text_token_id):\n",
    "\n",
    "    loss= 0\n",
    "\n",
    "    for i in range(len(original_caption)):\n",
    "\n",
    "        # remove all end of text tokens from the right in original caption\n",
    "        trim_index = 0\n",
    "        for j in range(len(original_caption[i])-1, -1, -1):\n",
    "            if original_caption[i][j] != end_of_text_token_id:\n",
    "                trim_index = j\n",
    "                break\n",
    "        trim_index += 1\n",
    "\n",
    "        # Trim the trailing spaces\n",
    "        trimmed_original = original_caption[i][:trim_index]\n",
    "        trimmed_reconstructed = reconstructed_caption[i][:trim_index]\n",
    "\n",
    "        # Calculate the loss (assuming cross-entropy loss)\n",
    "        loss += ae_criterion1(trimmed_reconstructed, trimmed_original)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/wd/7s_rgclx5rlc79rrjnspznh00000gn/T/ipykernel_26679/3431795457.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/wd/7s_rgclx5rlc79rrjnspznh00000gn/T/ipykernel_26679/3431795457.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.captions[idx])\n",
      "Epoch 0: 100%|██████████| 1/1 [00:41<00:00, 41.09s/it, ae_loss=110]\n",
      "Epoch 1: 100%|██████████| 1/1 [01:19<00:00, 79.66s/it, ae_loss=127]\n",
      "Epoch 2: 100%|██████████| 1/1 [01:12<00:00, 72.72s/it, ae_loss=226]\n",
      "Epoch 3: 100%|██████████| 1/1 [01:18<00:00, 78.00s/it, ae_loss=213]\n",
      "Epoch 4: 100%|██████████| 1/1 [01:15<00:00, 75.72s/it, ae_loss=114]\n",
      "Epoch 5: 100%|██████████| 1/1 [01:07<00:00, 67.53s/it, ae_loss=103]\n",
      "Epoch 6: 100%|██████████| 1/1 [01:15<00:00, 75.97s/it, ae_loss=86.3]\n",
      "Epoch 7: 100%|██████████| 1/1 [01:04<00:00, 64.64s/it, ae_loss=83.9]\n",
      "Epoch 8: 100%|██████████| 1/1 [01:15<00:00, 75.32s/it, ae_loss=85.7]\n",
      "Epoch 9: 100%|██████████| 1/1 [01:03<00:00, 63.02s/it, ae_loss=85]\n",
      "Epoch 10: 100%|██████████| 1/1 [01:01<00:00, 61.82s/it, ae_loss=78.2]\n",
      "Epoch 11: 100%|██████████| 1/1 [05:34<00:00, 334.09s/it, ae_loss=67.4]\n",
      "Epoch 12: 100%|██████████| 1/1 [02:01<00:00, 121.15s/it, ae_loss=57]\n",
      "Epoch 13: 100%|██████████| 1/1 [04:02<00:00, 242.62s/it, ae_loss=50.7]\n",
      "Epoch 14: 100%|██████████| 1/1 [08:22<00:00, 502.83s/it, ae_loss=54.5]\n",
      "Epoch 15: 100%|██████████| 1/1 [08:50<00:00, 530.82s/it, ae_loss=61.2]\n",
      "Epoch 16: 100%|██████████| 1/1 [01:28<00:00, 88.93s/it, ae_loss=58.1]\n",
      "Epoch 17: 100%|██████████| 1/1 [02:33<00:00, 153.25s/it, ae_loss=49.7]\n",
      "Epoch 18: 100%|██████████| 1/1 [01:14<00:00, 74.55s/it, ae_loss=44.3]\n",
      "Epoch 19: 100%|██████████| 1/1 [01:08<00:00, 68.26s/it, ae_loss=44.7]\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning using custom loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "vlm_lr = 5e-5 # TODO: find a good learning rate\n",
    "ae_lr = 1e-3 # TODO: find a good learning rate\n",
    "num_epochs = 20 # TODO: find a good number of epochs\n",
    "\n",
    "vlm_optimizer = AdamW([param for param in model.parameters() if param.requires_grad], lr=vlm_lr)\n",
    "ae_optimizer = optim.Adam(autoencoder.parameters(), lr=ae_lr) \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Fine tune VLM with custom loss\n",
    "        # Forward pass\n",
    "        # model.zero_grad()\n",
    "        # outputs = model(**batch)\n",
    "        # vlm_loss = custom_loss(outputs, batch, lora_layers, autoencoder)\n",
    "        # # Backward pass and optimization\n",
    "        # vlm_optimizer.zero_grad()\n",
    "        # vlm_loss.backward()\n",
    "        # vlm_optimizer.step()\n",
    "\n",
    "        # Train the autoencoder\n",
    "        autoencoder.zero_grad()\n",
    "        captions = batch['labels']\n",
    "        compressed_captions = autoencoder.encode(captions)\n",
    "        reconstructed_captions = autoencoder.decode(compressed_captions)\n",
    "        # reconstructed_flat = reconstructed_captions.view(-1, reconstructed_captions.size(-1))\n",
    "        # captions_flat = captions.view(-1)\n",
    "        end_of_text_token_id = tokenizer.encode('<|endoftext|>')[0]\n",
    "        ae_loss = ae_criterion2(reconstructed_captions, captions, end_of_text_token_id)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        # TODO: change loss as combination of vlm_loss and ae_loss instead of individual losses\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        # loop.set_postfix(vlm_loss=vlm_loss.item(), ae_loss=ae_loss.item())\n",
    "        loop.set_postfix( ae_loss=ae_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to save the model if it doesn't exist\n",
    "if not os.path.exists(\"models_auto_compress_online_data\"):\n",
    "    os.mkdir(\"models_auto_compress_online_data\")\n",
    "# save model checkpoint to models directory using current timestamp and date\n",
    "torch.save(model.state_dict(), f\"models_auto_compress_online_data/{time.strftime('%Y%m%d-%H%M%S')}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load latest model checkpoint among all the saved models\n",
    "latest_model = torch.load(max(glob.glob('models_auto_compress_online_data/*.pth'), key=os.path.getctime))\n",
    "# load the model with the latest checkpoint\n",
    "model.load_state_dict(latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   64,  4077,  7779, 19584,  1097,   319,   257,  4675,  1306,   284,\n",
      "           257, 13990,   220, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 351  351  351  351  351  351 7779  351  284  351  351  351  351 7779\n",
      "   351  351  351 7779  351 7779  284  351  351  284 7779  351  351  351\n",
      "   351  351  351  351  351  351 7779  351   64  351  351  351 7779  351\n",
      "  7779  351 7779  351  284  351  351  351  351  351  351  351  351  351\n",
      "   351 7779  351  351  351  351 7779  351  351 7779 7779  351  351  351\n",
      "   351 7779  351  351 7779 7779  351  351  351  351  351  351  351  351\n",
      "  7779 7779 7779  351  351  351  351  351  351  351 7779  351  351  351\n",
      "   351  351  284 7779  351  351 7779  351  351 7779  351  351  351   64\n",
      "   351 7779  351  351  351  351  284   64  351  351  351  351  351  351\n",
      "   351  351]]\n",
      "tensor([[   64,   582,  6155,   866,   257,  4675,   351,   257,  1097,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 351  351  351  351  351  351  351  351  351  351  351  351  351  351\n",
      "   351   64  351  351  351  351  351  351  262  351  351  351  351  351\n",
      "  2712  351  351  351  351  351  351  351 7779  351  351  351  351 7779\n",
      "   351  351  351  351 7779 7779  351  351  351  351 7779  351  351  351\n",
      "   351 7779  351  351 7779  351  351 7779  351  351  351  351  351  351\n",
      "  7779 7779 7779  351  351  262  351  351  351  351  351  351  351  351\n",
      "   351 7779  351  351  284 7779  351  351  351  351  351 7779  351  351\n",
      "  7779  351  351  351  351  351  351  351  351  351  262 7779  351  351\n",
      "   351  351  351  351  351   64  351  351  351  351  351  351  351  351\n",
      "   262  351]]\n",
      "tensor([[   64,  9283,  2137, 25635,   257,  7365,   379,   257,  2613,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 351  351  351  351  351  351  351  351  351  351  351  351  351  351\n",
      "  7779  351  351  351  351  351  351  351 7779 7779  351  351  351  351\n",
      "   351  351  351   64 7779  351  351 2712  351  351  284  351   64  351\n",
      "    64  262  351  351  351  351  351 7779  351  284  351  351  351  351\n",
      "   351  351 7779  351  351  351 7779 7779  262  351 7779 7779  351  351\n",
      "   351  351  351  351  351  351 7779  351  351 7779  351  351  351  351\n",
      "   351  351  351  351 7779  351  351  351  351  351  351  351  351  262\n",
      "   351  351  351  351  351  351  351  351  351  351  351 7779 7779  351\n",
      "  7779  351  351  351  351 7779  351 7779  351  351  351 7779  351   64\n",
      "   351  351]]\n",
      "tensor([[   64,  9875,  5055,   319,   257,  8701,  5017,  2214,   220, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 284  351   64  351  262  351  351  351  351  351  284  351 7779  351\n",
      "   351  351   64 7779  351  351  351  351 7779 7779  351 2712  351  351\n",
      "   351  351  351  351  351 7779  351  351  351  351  351 7779  351  351\n",
      "  7779 7779  262  351  351  351  351  351  351  351  351  351 7779  351\n",
      "  7779  351 7779  284  351  351 2712  351  351 7779 7779  351  351 7779\n",
      "   351  351  351  351  351  351  351  351  351  351  351   64  351 7779\n",
      "   351  351 7779  351 7779  351  351  351  351  351  351 7779 7779  351\n",
      "   351   64  262  351  351  351  351  351  351  351  351  351  351  351\n",
      "   351  351  351  351  351  284 7779  351   64  351  262  284   64  351\n",
      "  7779  351]]\n",
      "tensor([[   64,  2042,  3290,  5586,   287,   262,   736,   286,   257,  1097,\n",
      "           220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 351  351 2712  351 2712  351  351  351  351 7779  351  351  351 7779\n",
      "   351  351  351  284  351  351  351  351  351  351  351  351  351 7779\n",
      "   351   64  262   64  351  351   64 7779  351 7779 7779  351 7779  351\n",
      "   351  351  351  351  351  284  351  284 7779  351  351 7779 7779  262\n",
      "   351 7779  351 7779  351  351   64 2712 7779  351  351 7779 7779  262\n",
      "   351   64  351 7779   64  351  351  351  351  351  351  351  351  351\n",
      "   351  351 7779  351  351 7779  284  351  351  351  351  351  351 7779\n",
      "  7779  351  351 7779  284  351 7779  262 7779  351  351  351  351 7779\n",
      "   351  351 7779  351  351  351  351  351  262  351   64  351  351 7779\n",
      "   351 7779]]\n",
      "tensor([[   64,   582,   287,   257,  6050,   290,  9839,   351,   257,  9563,\n",
      "          9839,   220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 262  351  351  351  351  351  351  351  351   64  351  351  351  284\n",
      "   351  351  351 7779  351 7779  351  351  351 7779 7779 2712   64  351\n",
      "   351 7779  351 7779  351  351 7779  351  351  351  351  262 7779 7779\n",
      "   351  351  262  351  351  351  351 7779  351 7779 7779  351  351  351\n",
      "   351  262 7779  284  351  351  351 7779  351  351  351  351 7779  351\n",
      "   351  351  351  351  351  351  284  351  351  351  351  262  351  351\n",
      "   351  351  351  351  351 7779 7779  351   64   64  351  351  351  351\n",
      "    64 7779  351  351  351  351  351  284  351  262  351  351  351  351\n",
      "  7779  351  351 7779  351 7779  351  351  284  351 7779  351  351  351\n",
      "   351  351]]\n",
      "tensor([[   64, 17423,  2119,  3084,   351,   257,  1588,  9396,   286,  2057,\n",
      "           220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 284  351 7779  351  351  262  351  351  351  351  351  351  351  351\n",
      "   351  351 7779  351  351  351  351 7779   64 7779  351  351 7779  351\n",
      "   262  351  351  351  262  351  351  351 7779  351  351  284  284  351\n",
      "   351  262  351  351  351  351  351  262  351  351  351 7779  351  351\n",
      "   351  351  351  351  351  262  351  351  351  351 7779  351   64  351\n",
      "   351  351  351  351  351  351  351   64  351 7779  351 7779  351  351\n",
      "   351  351  351  351  351  351  351  284 7779  351  351  351  351 7779\n",
      "   351  351  351  351  351  351  351  351  351  351  351  351  351  351\n",
      "   351  351  351  262  351  351 7779  351  351  351  351  351  351  351\n",
      "   351  351]]\n",
      "tensor([[11545,  6510,   389,  2712,   257,   983,   351,   257, 10047,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 284 7779  351  351  284  351  351  351  351  351  351  351 7779  351\n",
      "   351  351  351  351  351  351  351 7779  351  351 7779 7779  351 7779\n",
      "   351  351  351  351  351  351   64 7779  351  351  351 7779 7779  351\n",
      "  7779  351  262  351 7779 7779 7779  351  351 7779  351 7779   64  351\n",
      "   284  262  351  351  351  351  351  262 7779  351  351 7779   64  351\n",
      "   351 7779  351  351 2712 7779 7779  351  351  351 7779  351   64  351\n",
      "   351  351  351  351  351 7779 7779 7779  351  351  262  351  351  351\n",
      "   351  351  351 7779  351 7779  351  351  351  351  351  351  351  351\n",
      "  7779  351  351  351  351  351 7779 7779  351 7779  351  351 7779  351\n",
      "   351  351]]\n",
      "tensor([[   64,   582,  9008,   257, 20790,  2613,   351,   257, 37391,   220,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[ 351  351  351 7779 7779  351 7779  351  351 7779 7779  351  351  351\n",
      "   351  351  351  351 7779  351 7779  351  351   64  351  351  351  351\n",
      "  7779  351  351  351  351 7779  351 7779 7779  351  351  351  351  351\n",
      "   351 7779  351 7779  351 7779  284  351  351  351  351  351  351  351\n",
      "   351  351  351  262 7779 7779 7779  351  351 7779   64  351  351   64\n",
      "  7779 7779 7779 7779  351 7779  351  351 7779  351  351  351 7779  284\n",
      "   351  351  351 7779   64 7779  351  351  351  351  351  351 7779  351\n",
      "   351  351 7779  351  351  351  351  351  351  262 7779  351  351  351\n",
      "  7779  351 7779  351 7779  351  284  351  262  351  351  351  351  284\n",
      "   351  351]]\n",
      "tensor([[   64,   582,   287,   257,  2415,   290,   257,   582,  2712,   257,\n",
      "           983,   286,  9283,   220, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[7779  351  351 7779  351  351   64  351 2712   64  351  351  351 7779\n",
      "   351 7779 7779  351  351  351  351 7779  351  351  262  351   64  351\n",
      "   351  351 7779  351 7779  351  351  351  351  351  351  351 7779  351\n",
      "   351  351  351 7779  351 7779  351  351   64  351 7779  351  262  262\n",
      "   351 7779 7779  351  351  351  351  351  351  351 7779  351  351 7779\n",
      "   351  351  351  351 7779  262  351  351  351  351  351  351   64  351\n",
      "   351  351  351  351  351  351 7779 7779  351  351 7779 7779  284  351\n",
      "   351  351 7779 7779  351 7779  284  351  351 7779  351  351  351 7779\n",
      "   351  351 7779 7779  351  351  284  351  351  351  351  351  351  351\n",
      "  7779   64]]\n"
     ]
    }
   ],
   "source": [
    "# Generate captions for the test dataset\n",
    "generated_captions_custom_model = []\n",
    "generated_captions_custom_model_pre_compression = []\n",
    "# Iterate over the dataset and generate captions\n",
    "for data in dataset:\n",
    "    image = data['image']\n",
    "    # use autoencoder to encode and decode the caption\n",
    "    caption = generate_caption(image)\n",
    "    generated_captions_custom_model_pre_compression.append(caption)\n",
    "    caption = tokenizer(caption, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    caption = caption['input_ids']\n",
    "    caption = caption.to(device)\n",
    "    print (caption)\n",
    "    compressed_caption = autoencoder.encode(caption)\n",
    "    compressed_caption = compressed_caption.to(device)\n",
    "    # print (compressed_caption)\n",
    "    reconstructed_caption = autoencoder.decode(compressed_caption)\n",
    "    reconstructed_caption = reconstructed_caption.to(device)\n",
    "    reconstructed_caption = reconstructed_caption.cpu()\n",
    "    reconstructed_caption = reconstructed_caption.detach().numpy()\n",
    "    reconstructed_caption = np.argmax(reconstructed_caption, axis=2)\n",
    "    print (reconstructed_caption)\n",
    "    reconstructed_caption = tokenizer.decode(reconstructed_caption[0], skip_special_tokens=True)\n",
    "    generated_captions_custom_model.append(reconstructed_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode compressed dictionary word using manual huffman encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace compressed_dict words occurring in the generated_captions_custom_model with their corresponding huffman encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare encoded generated_captions_custom_model + huffman encoding dictionary information with the original generated_captions to calculate compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a green truck parked next to a curb  a green truck parked car on a street next to a fence   with with with with with with truck with to with with with with truck with with with truck with truck to with with to truck with with with with with with with with with truck witha with with with truck with truck with truck with to with with with with with with with with with with truck with with with with truck with with truck truck with with with with truck with with truck truck with with with with with with with with truck truck truck with with with with with with with truck with with with with with to truck with with truck with with truck with with witha with truck with with with with toa with with with with with with with with\n",
      "a man is walking down the street with a skateboard  a man walking down a street with a car   with with with with with with with with with with with with with with witha with with with with with with the with with with with with playing with with with with with with with truck with with with with truck with with with with truck truck with with with with truck with with with with truck with with truck with with truck with with with with with with truck truck truck with with the with with with with with with with with with truck with with to truck with with with with with truck with with truck with with with with with with with with with the truck with with with with with with witha with with with with with with with with the with\n",
      "a baseball player swinging a bat at a ball  a baseball player swinging a bat at a ball   with with with with with with with with with with with with with with truck with with with with with with with truck truck with with with with with with witha truck with with playing with with to witha witha the with with with with with truck with to with with with with with with truck with with with truck truck the with truck truck with with with with with with with with truck with with truck with with with with with with with with truck with with with with with with with with the with with with with with with with with with with with truck truck with truck with with with with truck with truck with with with truck witha with with\n",
      "a cow is standing in a field of grass  a cow standing on a grass covered field   to witha with the with with with with with to with truck with with witha truck with with with with truck truck with playing with with with with with with with truck with with with with with truck with with truck truck the with with with with with with with with with truck with truck with truck to with with playing with with truck truck with with truck with with with with with with with with with with witha with truck with with truck with truck with with with with with with truck truck with witha the with with with with with with with with with with with with with with with with to truck witha with the toa with truck with\n",
      "a black dog sitting in the back of a truck  a black dog sitting in the back of a car   with with playing with playing with with with with truck with with with truck with with with to with with with with with with with with with truck witha thea with witha truck with truck truck with truck with with with with with with to with to truck with with truck truck the with truck with truck with witha playing truck with with truck truck the witha with trucka with with with with with with with with with with with truck with with truck to with with with with with with truck truck with with truck to with truck the truck with with with with truck with with truck with with with with with the witha with with truck with truck\n",
      "a man wearing a bow tie and glasses  a man in a suit and tie with a bow tie   the with with with with with with with witha with with with to with with with truck with truck with with with truck truck playinga with with truck with truck with with truck with with with with the truck truck with with the with with with with truck with truck truck with with with with the truck to with with with truck with with with with truck with with with with with with with to with with with with the with with with with with with with truck truck withaa with with with witha truck with with with with with to with the with with with with truck with with truck with truck with with to with truck with with with with with\n",
      "a dining room table with a large bowl of food  a dining room table with a large bowl of food   to with truck with with the with with with with with with with with with with truck with with with with trucka truck with with truck with the with with with the with with with truck with with to to with with the with with with with with the with with with truck with with with with with with with the with with with with truck witha with with with with with with with witha with truck with truck with with with with with with with with with to truck with with with with truck with with with with with with with with with with with with with with with with with the with with truck with with with with with with with with with\n",
      "a man standing next to a wall with a bunch of guitars  two boys are playing a game with a guitar   to truck with with to with with with with with with with truck with with with with with with with with truck with with truck truck with truck with with with with with witha truck with with with truck truck with truck with the with truck truck truck with with truck with trucka with to the with with with with with the truck with with trucka with with truck with with playing truck truck with with with truck witha with with with with with with truck truck truck with with the with with with with with with truck with truck with with with with with with with with truck with with with with with truck truck with truck with with truck with with with\n",
      "a man is playing tennis on a clay court  a man hitting a tennis ball with a racket   with with with truck truck with truck with with truck truck with with with with with with with truck with truck with witha with with with with truck with with with with truck with truck truck with with with with with with truck with truck with truck to with with with with with with with with with with the truck truck truck with with trucka with witha truck truck truck truck with truck with with truck with with with truck to with with with trucka truck with with with with with with truck with with with truck with with with with with with the truck with with with truck with truck with truck with to with the with with with with to with with\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# print generated_captions and generated_captions_custom_model elementwise to compare the results\n",
    "for i in range(len(generated_captions)-1):\n",
    "    print (generated_captions[i], generated_captions_custom_model_pre_compression[i], generated_captions_custom_model[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improve autoencoder architecture, use better semantic meaning preserving metric instead of simply cross entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
