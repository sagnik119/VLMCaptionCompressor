{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing manual compression of image captions on stale (offline) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import fiftyone\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik/Library/Python/3.9/lib/python/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and its components\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 81434/81434 [00:00<00:00, 1191254.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset (for example, a subset of the COCO dataset)\n",
    "# TODO: Potential datasets with repititive nature that can be used: MS COCO, Flickr30k, Visual Genome, SBU Captions \n",
    "\n",
    "# load small part of the coco dataset from all the .jpg images in datasets/mscoco/test2015\n",
    "dataset = load_dataset(\"datasets/mscoco/test2015/\", split=\"test[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, max_length=128):\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(inputs[\"pixel_values\"], max_length=max_length)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataset and generate captions\n",
    "max_length = 128\n",
    "generated_captions = []\n",
    "\n",
    "for data in dataset:\n",
    "    image = data['image']\n",
    "    caption = generate_caption(image, max_length=max_length)\n",
    "    generated_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_encoding_dict(captions, encoding_dict):\n",
    "    for caption in captions:\n",
    "        words = caption.split()\n",
    "        encoding_dict.update(words) # purpose of update is to add the words to the dictionary if they don't exist\n",
    "    return encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dict = Counter() # Counter is a subclass of dictionary for counting hashable objects\n",
    "threshold = 2 # threshold for word frequency # TODO: find a good threshold\n",
    "\n",
    "update_encoding_dict(generated_captions, encoding_dict)\n",
    "\n",
    "# Optionally, create a more compressed form based on frequency\n",
    "compressed_dict = {word: idx for idx, (word, freq) in enumerate(encoding_dict.items()) if freq > threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, max_seq_length):\n",
    "        super(CaptionAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, embedding_dim) # input shape has to be (batch_size, sequence_length), output shape is (batch_size, sequence_length, embedding_dim)\n",
    "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True) # output shape is (batch_size, sequence_length, hidden_dim)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True) # output shape is (batch_size, sequence_length, hidden_dim)\n",
    "        self.decoder_output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def encode(self, captions):\n",
    "        embedded = self.encoder_embedding(captions)\n",
    "        encoded, _ = self.encoder_rnn(embedded)\n",
    "        return encoded[:, -1, :]\n",
    "\n",
    "    def decode(self, encoded):\n",
    "        # Repeat the encoded state across the sequence length\n",
    "        repeated_encoded = encoded.unsqueeze(1).repeat(1, self.max_seq_length, 1) \n",
    "        decoded, _ = self.decoder_rnn(repeated_encoded)\n",
    "        output = self.decoder_output(decoded)\n",
    "        return output\n",
    "\n",
    "    def forward(self, captions):\n",
    "        encoded = self.encode(captions)\n",
    "        decoded = self.decode(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, captions):\n",
    "        self.encodings = encodings\n",
    "        self.captions = captions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.captions[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `dataset` is your dataset containing images and captions\n",
    "images = [data['image'] for data in dataset]\n",
    "captions = generated_captions\n",
    "\n",
    "# Process images and captions\n",
    "inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "outputs = tokenizer(captions, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Assuming 'captions' is a tensor of tokenized captions generated by VLM\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 50257\n",
    "hidden_dim = 512\n",
    "max_seq_length = 128\n",
    "autoencoder = CaptionAutoencoder(vocab_size, embedding_dim, hidden_dim, max_seq_length)\n",
    "autoencoder_output = autoencoder(outputs[\"input_ids\"])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = CaptionDataset(inputs, outputs[\"input_ids\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_weight, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_weight = original_weight\n",
    "        self.rank = rank\n",
    "        self.U = nn.Parameter(torch.Tensor(self.original_weight.size(0), self.rank))\n",
    "        self.V = nn.Parameter(torch.Tensor(self.rank, self.original_weight.size(1)))\n",
    "        nn.init.xavier_uniform_(self.U)\n",
    "        nn.init.xavier_uniform_(self.V)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.original_weight + self.U @ self.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the first attention layer of the encoder\n",
    "# TODO: Try modifying other layers as well and check the results\n",
    "lora_layers = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_weight = model.encoder.encoder.layer[0].attention.output.dense.weight\n",
    "    lora_layer = LoRALayer(original_weight, rank=10).forward()  # Choose an appropriate rank\n",
    "    # assign the new layer to the model\n",
    "    model.encoder.encoder.layer[0].attention.output.dense.weight.copy_(lora_layer)\n",
    "    # add the layer of the model to the list of LoRA layers\n",
    "    lora_layers.append(model.encoder.encoder.layer[0].attention.output.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lora_param(param, lora_layer):\n",
    "    # check if the parameter is part of the LoRA layer\n",
    "    print (lora_layer.parameters())\n",
    "    print (\"nuj\")\n",
    "    print (param)\n",
    "    return param in lora_layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(outputs, batch, lora_layers, autoencoder, standard_lambda_val = 1, lora_lambda_val = 0.01, compression_lambda_val = 0.0001):\n",
    "    # Standard captioning loss\n",
    "    standard_loss = outputs.loss\n",
    "\n",
    "    # Autoencoder compression reward\n",
    "    captions = batch['labels']\n",
    "    compressed_captions = autoencoder.encode(captions)\n",
    "    # Measure the sparsity of the compressed representation (e.g., using L1 norm) # TODO: Try other measures\n",
    "    compression_reward = torch.norm(compressed_captions, p=1)\n",
    "    # Adjust the reward: lower norm (more sparse) should lead to lower loss (higher reward)\n",
    "    compression_loss = -compression_reward\n",
    "\n",
    "    # Optionally, add a term for LoRA regularization if needed\n",
    "    lora_regularization = 0\n",
    "    # for param in model.parameters():\n",
    "    #     for lora_layer in lora_layers:\n",
    "    #         if is_lora_param(param, lora_layer):\n",
    "    #             lora_regularization += torch.norm(param)\n",
    "    return standard_lambda_val* standard_loss + compression_lambda_val * compression_loss + lora_lambda_val * lora_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_criterion2 (reconstructed_caption_flat, original_caption_flat, end_of_text_token_id):\n",
    "    \n",
    "    # Convert tensors to numpy arrays for easier processing (optional)\n",
    "    original_caption_flat = original_caption_flat.numpy()\n",
    "    reconstructed_caption_flat = reconstructed_caption_flat.numpy()\n",
    "\n",
    "    # Find the index of the first space token in the reverse of original_caption\n",
    "    trim_index = len(original_caption_flat) - np.argmax(original_caption_flat[::-1] != end_of_text_token_id)\n",
    "\n",
    "    # Trim the trailing spaces\n",
    "    trimmed_original = original_caption[:trim_index]\n",
    "    trimmed_reconstructed = reconstructed_caption[:trim_index]\n",
    "\n",
    "    # Calculate the loss (assuming cross-entropy loss)\n",
    "    loss = cross_entropy_loss(trimmed_original, trimmed_reconstructed)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/wd/7s_rgclx5rlc79rrjnspznh00000gn/T/ipykernel_40145/3431795457.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/wd/7s_rgclx5rlc79rrjnspznh00000gn/T/ipykernel_40145/3431795457.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.captions[idx])\n",
      "Epoch 0: 100%|██████████| 1/1 [00:35<00:00, 35.47s/it, ae_loss=8.43, vlm_loss=1.28]\n",
      "Epoch 1: 100%|██████████| 1/1 [00:43<00:00, 43.13s/it, ae_loss=7.51, vlm_loss=0.574]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:43<00:00, 43.42s/it, ae_loss=6.86, vlm_loss=0.261]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:43<00:00, 43.63s/it, ae_loss=6.31, vlm_loss=-.0241]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:39<00:00, 39.57s/it, ae_loss=5.81, vlm_loss=-.211]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:41<00:00, 41.50s/it, ae_loss=5.34, vlm_loss=-.302]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:38<00:00, 38.57s/it, ae_loss=4.9, vlm_loss=-.353]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:42<00:00, 42.62s/it, ae_loss=4.47, vlm_loss=-.394]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:39<00:00, 39.21s/it, ae_loss=4.05, vlm_loss=-.414]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:42<00:00, 42.46s/it, ae_loss=3.65, vlm_loss=-.41]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:40<00:00, 40.96s/it, ae_loss=3.29, vlm_loss=-.423]\n",
      "Epoch 11: 100%|██████████| 1/1 [00:38<00:00, 38.72s/it, ae_loss=2.96, vlm_loss=-.421]\n",
      "Epoch 12: 100%|██████████| 1/1 [00:40<00:00, 40.37s/it, ae_loss=2.66, vlm_loss=-.435]\n",
      "Epoch 13: 100%|██████████| 1/1 [00:37<00:00, 37.72s/it, ae_loss=2.38, vlm_loss=-.428]\n",
      "Epoch 14: 100%|██████████| 1/1 [00:42<00:00, 42.57s/it, ae_loss=2.14, vlm_loss=-.436]\n",
      "Epoch 15: 100%|██████████| 1/1 [00:42<00:00, 42.09s/it, ae_loss=1.93, vlm_loss=-.433]\n",
      "Epoch 16: 100%|██████████| 1/1 [00:39<00:00, 39.32s/it, ae_loss=1.76, vlm_loss=-.429]\n",
      "Epoch 17: 100%|██████████| 1/1 [00:45<00:00, 45.59s/it, ae_loss=1.61, vlm_loss=-.428]\n",
      "Epoch 18: 100%|██████████| 1/1 [00:38<00:00, 38.40s/it, ae_loss=1.48, vlm_loss=-.441]\n",
      "Epoch 19: 100%|██████████| 1/1 [00:40<00:00, 40.89s/it, ae_loss=1.38, vlm_loss=-.438]\n",
      "Epoch 20: 100%|██████████| 1/1 [00:44<00:00, 44.20s/it, ae_loss=1.29, vlm_loss=-.439]\n",
      "Epoch 21: 100%|██████████| 1/1 [00:39<00:00, 39.85s/it, ae_loss=1.22, vlm_loss=-.442]\n",
      "Epoch 22: 100%|██████████| 1/1 [02:09<00:00, 129.02s/it, ae_loss=1.17, vlm_loss=-.444]\n",
      "Epoch 23: 100%|██████████| 1/1 [01:19<00:00, 79.10s/it, ae_loss=1.12, vlm_loss=-.445]\n",
      "Epoch 24: 100%|██████████| 1/1 [35:05<00:00, 2105.14s/it, ae_loss=1.08, vlm_loss=-.443]\n",
      "Epoch 25: 100%|██████████| 1/1 [09:02<00:00, 543.00s/it, ae_loss=1.04, vlm_loss=-.445]\n",
      "Epoch 26: 100%|██████████| 1/1 [00:41<00:00, 41.95s/it, ae_loss=1.01, vlm_loss=-.445]\n",
      "Epoch 27: 100%|██████████| 1/1 [00:42<00:00, 42.29s/it, ae_loss=0.987, vlm_loss=-.448]\n",
      "Epoch 28: 100%|██████████| 1/1 [00:39<00:00, 39.17s/it, ae_loss=0.964, vlm_loss=-.453]\n",
      "Epoch 29: 100%|██████████| 1/1 [00:40<00:00, 40.06s/it, ae_loss=0.943, vlm_loss=-.45]\n",
      "Epoch 30: 100%|██████████| 1/1 [00:42<00:00, 42.88s/it, ae_loss=0.925, vlm_loss=-.453]\n",
      "Epoch 31: 100%|██████████| 1/1 [00:38<00:00, 38.05s/it, ae_loss=0.909, vlm_loss=-.452]\n",
      "Epoch 32: 100%|██████████| 1/1 [00:39<00:00, 39.64s/it, ae_loss=0.894, vlm_loss=-.453]\n",
      "Epoch 33: 100%|██████████| 1/1 [00:38<00:00, 38.54s/it, ae_loss=0.881, vlm_loss=-.453]\n",
      "Epoch 34: 100%|██████████| 1/1 [00:40<00:00, 40.81s/it, ae_loss=0.869, vlm_loss=-.461]\n",
      "Epoch 35: 100%|██████████| 1/1 [00:38<00:00, 38.74s/it, ae_loss=0.857, vlm_loss=-.45]\n",
      "Epoch 36: 100%|██████████| 1/1 [00:41<00:00, 41.51s/it, ae_loss=0.847, vlm_loss=-.458]\n",
      "Epoch 37: 100%|██████████| 1/1 [00:38<00:00, 38.50s/it, ae_loss=0.836, vlm_loss=-.458]\n",
      "Epoch 38: 100%|██████████| 1/1 [00:40<00:00, 40.91s/it, ae_loss=0.827, vlm_loss=-.457]\n",
      "Epoch 39: 100%|██████████| 1/1 [00:42<00:00, 42.52s/it, ae_loss=0.818, vlm_loss=-.459]\n",
      "Epoch 40: 100%|██████████| 1/1 [00:38<00:00, 38.49s/it, ae_loss=0.809, vlm_loss=-.459]\n",
      "Epoch 41: 100%|██████████| 1/1 [00:38<00:00, 38.61s/it, ae_loss=0.801, vlm_loss=-.457]\n",
      "Epoch 42: 100%|██████████| 1/1 [00:37<00:00, 37.71s/it, ae_loss=0.792, vlm_loss=-.464]\n",
      "Epoch 43: 100%|██████████| 1/1 [00:41<00:00, 41.86s/it, ae_loss=0.785, vlm_loss=-.461]\n",
      "Epoch 44: 100%|██████████| 1/1 [00:38<00:00, 38.83s/it, ae_loss=0.777, vlm_loss=-.463]\n",
      "Epoch 45: 100%|██████████| 1/1 [00:43<00:00, 43.09s/it, ae_loss=0.77, vlm_loss=-.462]\n",
      "Epoch 46: 100%|██████████| 1/1 [00:41<00:00, 41.59s/it, ae_loss=0.763, vlm_loss=-.464]\n",
      "Epoch 47: 100%|██████████| 1/1 [00:39<00:00, 39.16s/it, ae_loss=0.757, vlm_loss=-.466]\n",
      "Epoch 48: 100%|██████████| 1/1 [00:40<00:00, 40.99s/it, ae_loss=0.75, vlm_loss=-.465]\n",
      "Epoch 49: 100%|██████████| 1/1 [00:38<00:00, 38.54s/it, ae_loss=0.744, vlm_loss=-.463]\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning using custom loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "vlm_lr = 5e-6 # TODO: find a good learning rate\n",
    "ae_lr = 1e-4 # TODO: find a good learning rate\n",
    "num_epochs = 50 # TODO: find a good number of epochs\n",
    "\n",
    "vlm_optimizer = AdamW([param for param in model.parameters() if param.requires_grad], lr=vlm_lr)\n",
    "ae_optimizer = optim.Adam(autoencoder.parameters(), lr=ae_lr)  \n",
    "ae_criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Fine tune VLM with custom loss\n",
    "        # Forward pass\n",
    "        model.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        vlm_loss = custom_loss(outputs, batch, lora_layers, autoencoder)\n",
    "        # Backward pass and optimization\n",
    "        vlm_optimizer.zero_grad()\n",
    "        vlm_loss.backward()\n",
    "        vlm_optimizer.step()\n",
    "\n",
    "        # Train the autoencoder\n",
    "        autoencoder.zero_grad()\n",
    "        captions = batch['labels']\n",
    "        compressed_captions = autoencoder.encode(captions)\n",
    "        reconstructed_captions = autoencoder.decode(compressed_captions)\n",
    "        reconstructed_flat = reconstructed_captions.view(-1, reconstructed_captions.size(-1))\n",
    "        captions_flat = captions.view(-1)\n",
    "        end_of_text_token_id = tokenizer.encode('<|endoftext|>')\n",
    "        ae_loss = ae_criterion2(reconstructed_flat, captions_flat, end_of_text_token_id)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        # TODO: change loss as combination of vlm_loss and ae_loss instead of individual losses\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(vlm_loss=vlm_loss.item(), ae_loss=ae_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to save the model if it doesn't exist\n",
    "if not os.path.exists(\"models_auto_compress_online_data\"):\n",
    "    os.mkdir(\"models_auto_compress_online_data\")\n",
    "# save model checkpoint to models directory using current timestamp and date\n",
    "torch.save(model.state_dict(), f\"models_auto_compress_online_data/{time.strftime('%Y%m%d-%H%M%S')}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load latest model checkpoint among all the saved models\n",
    "latest_model = torch.load(max(glob.glob('models_auto_compress_online_data/*.pth'), key=os.path.getctime))\n",
    "# load the model with the latest checkpoint\n",
    "model.load_state_dict(latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   64,  4077,  7779, 19584,  1306,   284,   257, 20799,   220, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
      "[[50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      "  50256 50256 50256 50256 50256 50256 50256 50256]]\n"
     ]
    }
   ],
   "source": [
    "# Generate captions for the test dataset\n",
    "generated_captions_custom_model = []\n",
    "generated_captions_custom_model_pre_compression = []\n",
    "# Iterate over the dataset and generate captions\n",
    "for data in dataset:\n",
    "    image = data['image']\n",
    "    # use autoencoder to encode and decode the caption\n",
    "    caption = generate_caption(image)\n",
    "    generated_captions_custom_model_pre_compression.append(caption)\n",
    "    caption = tokenizer(caption, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    caption = caption['input_ids']\n",
    "    caption = caption.to(device)\n",
    "    print (caption)\n",
    "    compressed_caption = autoencoder.encode(caption)\n",
    "    compressed_caption = compressed_caption.to(device)\n",
    "    # print (compressed_caption)\n",
    "    reconstructed_caption = autoencoder.decode(compressed_caption)\n",
    "    reconstructed_caption = reconstructed_caption.to(device)\n",
    "    reconstructed_caption = reconstructed_caption.cpu()\n",
    "    reconstructed_caption = reconstructed_caption.detach().numpy()\n",
    "    reconstructed_caption = np.argmax(reconstructed_caption, axis=2)\n",
    "    print (reconstructed_caption)\n",
    "    reconstructed_caption = tokenizer.decode(reconstructed_caption[0], skip_special_tokens=True)\n",
    "    generated_captions_custom_model.append(reconstructed_caption)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode compressed dictionary word using manual huffman encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace compressed_dict words occurring in the generated_captions_custom_model with their corresponding huffman encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare encoded generated_captions_custom_model + huffman encoding dictionary information with the original generated_captions to calculate compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a green truck parked next to a curb  a green truck parked next to a curb  \n",
      "a man is walking down the street with a skateboard  a man is walking down the street with a skateboard  \n",
      "a baseball player swinging a bat at a ball  a baseball player swinging a bat at a ball  \n",
      "a cow is standing in a field of grass  a cow standing in a field of grass and trees  \n",
      "a black dog sitting in the back of a truck  a black dog sitting in the back of a truck  \n",
      "a man wearing a bow tie and glasses  a man wearing a bow tie and a bow tie  \n",
      "a dining room table with a large bowl of food  a table with a bowl of food on it  \n",
      "a man standing next to a wall with a bunch of guitars  a man standing next to a wall with a guitar  \n",
      "a man is playing tennis on a clay court  a man holding a tennis racquet on a tennis court  \n",
      "a man and a woman playing a game of frisbee  a man and a woman playing a game of frisbee  \n"
     ]
    }
   ],
   "source": [
    "# print generated_captions and generated_captions_custom_model elementwise to compare the results\n",
    "for i in range(len(generated_captions)):\n",
    "    print (generated_captions[i], generated_captions_custom_model_pre_compression[i], generated_captions_custom_model[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
