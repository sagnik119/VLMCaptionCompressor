{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing manual compression of image captions on stale (offline) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from collections import Counter\n",
    "import fiftyone\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# uncommon features  - events of interest\n",
    "# loss less compression -  sudden more bits indicates anomaly can be flagged, alerts when anomaly detected - may shift to lossy video streaming\n",
    "# lossy compression of noisy data varying distortion rate - accuracy is increasing\n",
    "# video to video lossy reconstruction possibility\n",
    "# image frame to image frame on a need basis - human satisfaction metric, GPT based comparison, RLHF based comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik/Library/Python/3.9/lib/python/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and its components\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 81434/81434 [00:00<00:00, 486913.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset (for example, a subset of the COCO dataset)\n",
    "# TODO: Potential datasets with repititive nature that can be used: MS COCO, Flickr30k, Visual Genome, SBU Captions \n",
    "\n",
    "# load small part of the coco dataset from all the .jpg images in datasets/mscoco/test2015\n",
    "dataset = load_dataset(\"datasets/mscoco/test2015/\", split=\"test[:2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_with_logits(image, max_length=128):\n",
    "    # Prepare the inputs\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Perform a forward pass to get the logits\n",
    "        encoder_outputs = model.encoder(pixel_values=pixel_values)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Prepare decoder input_ids. Typically, you start with the start-of-sentence token\n",
    "        decoder_input_ids = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0).to(encoder_hidden_states.device)\n",
    "        decoder_attention_mask = torch.ones_like(decoder_input_ids)\n",
    "        \n",
    "        # Initialize an empty tensor for logits (for simplicity, accumulating logits for each step)\n",
    "        logits_list = []\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            decoder_outputs = model.decoder(input_ids=decoder_input_ids,\n",
    "                                            attention_mask=decoder_attention_mask,\n",
    "                                            encoder_hidden_states=encoder_hidden_states)\n",
    "            logits = decoder_outputs.logits[:, -1, :]  # Get the logits for the last token generated\n",
    "            logits_list.append(logits)\n",
    "            \n",
    "            predicted_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "            # Check if EOS token is generated\n",
    "            if predicted_id[0, 0] == tokenizer.eos_token_id:\n",
    "                print (\"EOS has been generated\")\n",
    "            \n",
    "            # Append predicted token ID to decoder_input_ids for generating next token\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, predicted_id], dim=-1)\n",
    "            decoder_attention_mask = torch.cat([decoder_attention_mask, torch.ones_like(predicted_id)], dim=-1)\n",
    "            \n",
    "        # Concatenate logits from each step to get the final logits tensor\n",
    "        # make all elements of logits_list 3D by adding a dimension in the middle\n",
    "        logits_list = [logits.unsqueeze(1) for logits in logits_list]\n",
    "        logits = torch.cat(logits_list, dim=1)\n",
    "        # Decode the generated token IDs to get the caption\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        caption = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "    return logits, predicted_ids, caption\n",
    "\n",
    "# Example usage\n",
    "# image: A PIL image or a tensor representing your input image\n",
    "# logits, predicted_ids, caption = generate_caption_with_logits(image, model, feature_extractor, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "EOS has been generated\n",
      "torch.Size([2, 128, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataset and generate captions\n",
    "generated_captions = []\n",
    "generated_logits = []\n",
    "generated_predicted_ids = []\n",
    "\n",
    "for data in dataset:\n",
    "    image = data['image']\n",
    "    logits, predicted_ids, caption = generate_caption_with_logits(image)\n",
    "    generated_captions.append(caption)\n",
    "    generated_logits.append(logits)\n",
    "    generated_predicted_ids.append(predicted_ids)\n",
    "\n",
    "# concatenate generated logits along first dimension to make 3D tensor\n",
    "generated_logits = torch.cat(generated_logits, dim=0)\n",
    "print (generated_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a green truck parked next to a curb a green truck parked next to a fence a green truck parked next to a fence a parking meter on a street a green truck parked next to a fence a green truck parked next to a fence a green truck parked next to a fence a green truck parked next to a fence a green truck parked next to a fence a green truck parked next to a fence a green truck parked next to a fence a green truck parked next to a fence a green truck parked next to a fence ',\n",
       " 'a man is walking down the street with a skateboard a man is crossing the street in front of a traffic light a man is crossing the street with a bicycle a man is crossing the street with a car a man is crossing the street with a car a man is crossing the street with a bike a man is crossing the street with a car a man is crossing the street with a car a man is crossing the street with a car a man is crossing the street with a car a man is crossing the street with a car a man']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_encoding_dict(captions, encoding_dict):\n",
    "    for caption in captions:\n",
    "        words = caption.split()\n",
    "        encoding_dict.update(words) # purpose of update is to add the words to the dictionary if they don't exist\n",
    "    return encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 49, 'green': 12, 'truck': 12, 'parked': 12, 'next': 12, 'to': 12, 'street': 12, 'man': 12, 'fence': 11, 'is': 11, 'the': 11, 'with': 10, 'crossing': 10, 'car': 7, 'curb': 1, 'parking': 1, 'meter': 1, 'on': 1, 'walking': 1, 'down': 1, 'skateboard': 1, 'in': 1, 'front': 1, 'of': 1, 'traffic': 1, 'light': 1, 'bicycle': 1, 'bike': 1})\n",
      "{'a': 1.4408984951547426, 'green': 2.847812143477369, 'truck': 2.847812143477369, 'parked': 2.847812143477369, 'next': 2.847812143477369, 'to': 2.847812143477369, 'curb': 5.332718793265369, 'fence': 2.9348235204669986, 'parking': 5.332718793265369, 'meter': 5.332718793265369, 'on': 5.332718793265369, 'street': 2.847812143477369, 'man': 2.847812143477369, 'is': 2.9348235204669986, 'walking': 5.332718793265369, 'down': 5.332718793265369, 'the': 2.9348235204669986, 'with': 3.0301337002713233, 'skateboard': 5.332718793265369, 'crossing': 3.0301337002713233, 'in': 5.332718793265369, 'front': 5.332718793265369, 'of': 5.332718793265369, 'traffic': 5.332718793265369, 'light': 5.332718793265369, 'bicycle': 5.332718793265369, 'car': 3.386808644210056, 'bike': 5.332718793265369}\n",
      "{'a': 0.02, 'green': 0.07692307692307693, 'truck': 0.07692307692307693, 'parked': 0.07692307692307693, 'next': 0.07692307692307693, 'to': 0.07692307692307693, 'curb': 0.5, 'fence': 0.08333333333333333, 'parking': 0.5, 'meter': 0.5, 'on': 0.5, 'street': 0.07692307692307693, 'man': 0.07692307692307693, 'is': 0.08333333333333333, 'walking': 0.5, 'down': 0.5, 'the': 0.08333333333333333, 'with': 0.09090909090909091, 'skateboard': 0.5, 'crossing': 0.09090909090909091, 'in': 0.5, 'front': 0.5, 'of': 0.5, 'traffic': 0.5, 'light': 0.5, 'bicycle': 0.5, 'car': 0.125, 'bike': 0.5}\n"
     ]
    }
   ],
   "source": [
    "encoding_dict = Counter() # Counter is a subclass of dictionary for counting hashable objects\n",
    "threshold = 0 # threshold for word frequency # TODO: find a good threshold\n",
    "\n",
    "update_encoding_dict(generated_captions, encoding_dict)\n",
    "\n",
    "print (encoding_dict)\n",
    "\n",
    "# Optionally, create a more compressed form based on frequency\n",
    "compressed_dict = {word: idx for idx, (word, freq) in enumerate(encoding_dict.items()) if freq > threshold}\n",
    "\n",
    "# Create the dictionary of entropy values from encoding_dict\n",
    "entropy_dict = {word: -np.log(encoding_dict[word] / sum(encoding_dict.values())) \n",
    "                for word in encoding_dict}\n",
    "\n",
    "print (entropy_dict)\n",
    "# print 1/elem for elem in encoding_dict.values()\n",
    "reciprocal_dict = {word: 1/(encoding_dict[word]+1) for word in encoding_dict}\n",
    "print (reciprocal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, captions):\n",
    "        self.encodings = encodings\n",
    "        self.captions = captions\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.captions[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `dataset` is your dataset containing images and captions\n",
    "images = [data['image'] for data in dataset]\n",
    "caption_ids = generated_predicted_ids\n",
    "\n",
    "# Process images and captions\n",
    "inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = CaptionDataset(inputs, caption_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_weight, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_weight = original_weight\n",
    "        self.rank = rank\n",
    "        self.U = nn.Parameter(torch.Tensor(self.original_weight.size(0), self.rank))\n",
    "        self.V = nn.Parameter(torch.Tensor(self.rank, self.original_weight.size(1)))\n",
    "        nn.init.xavier_uniform_(self.U)\n",
    "        nn.init.xavier_uniform_(self.V)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.original_weight + self.U @ self.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the first attention layer of the encoder\n",
    "# TODO: Try modifying other layers as well and check the results\n",
    "lora_layers = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_weight = model.encoder.encoder.layer[0].attention.output.dense.weight\n",
    "    lora_layer = LoRALayer(original_weight, rank=10).forward()  # Choose an appropriate rank\n",
    "    # assign the new layer to the model\n",
    "    model.encoder.encoder.layer[0].attention.output.dense.weight.copy_(lora_layer)\n",
    "    # add the layer of the model to the list of LoRA layers\n",
    "    lora_layers.append(model.encoder.encoder.layer[0].attention.output.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lora_param(param, lora_layer):\n",
    "    # check if the parameter is part of the LoRA layer\n",
    "    print (lora_layer.parameters())\n",
    "    print (\"nuj\")\n",
    "    print (param)\n",
    "    return param in lora_layer.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two extra dimensions to generated_logits\n",
    "generated_probs = F.softmax(generated_logits, dim=-1)\n",
    "generated_probs_expanded = generated_probs.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_elbo_difference (prob_differences, D):\n",
    "    sigma = 0.01\n",
    "    # reduce prob_differences to 4D from 5D by taking norm square along the last dimension\n",
    "    prob_differences = torch.norm(prob_differences, dim=-1)\n",
    "    print (prob_differences.shape)\n",
    "    # do elementwise for prob_differences: suqare\n",
    "    prob_differences = prob_differences**2\n",
    "    # take sum of all elements of prob_differences, hence scalar, then divide by 2*sigma^2*D\n",
    "    return torch.sum(prob_differences) / (2*sigma**2*D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_elbo_cross_entropy (prob_differences, D):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(outputs, batch, encoding_dict, lora_layers, lambda_val=1, lora_lambda_val = 0.01):\n",
    "    # Standard captioning loss\n",
    "    standard_loss = outputs.loss\n",
    "\n",
    "    # Additional compression loss\n",
    "    compression_loss = 0\n",
    "    # add two dimensions to output probs at 2 and 3\n",
    "    outputs_probs = F.softmax(outputs.logits, dim=-1)\n",
    "    outputs_probs_expanded = outputs_probs.squeeze(1).unsqueeze(2).unsqueeze(3)\n",
    "    prob_differences = generated_probs_expanded - outputs_probs_expanded\n",
    "    print (\"prob_differences.shape = \", outputs_probs.shape, generated_probs_expanded.shape, outputs_probs_expanded.shape, prob_differences.shape)\n",
    "    # calculate the compression loss\n",
    "    # find number of elements in generated_predicted_logits\n",
    "    D = generated_probs.numel()\n",
    "    compression_loss = lambda_val* calculate_entropy_elbo_difference (prob_differences, D)\n",
    "     \n",
    "\n",
    "    # Optionally, add a term for LoRA regularization if needed\n",
    "    lora_regularization = 0\n",
    "    # for param in model.parameters():\n",
    "    #     for lora_layer in lora_layers:\n",
    "    #         if is_lora_param(param, lora_layer):\n",
    "    #             lora_regularization += torch.norm(param)\n",
    "    print (standard_loss, compression_loss)\n",
    "\n",
    "    return standard_loss + compression_loss + lora_lambda_val * lora_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/7s_rgclx5rlc79rrjnspznh00000gn/T/ipykernel_40382/3431795457.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/var/folders/wd/7s_rgclx5rlc79rrjnspznh00000gn/T/ipykernel_40382/3431795457.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.captions[idx])\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(5.6478, grad_fn=<NllLossBackward0>) tensor(13.5347, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [02:21<00:00, 141.98s/it, loss=19.2]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(6.8851, grad_fn=<NllLossBackward0>) tensor(12.0999, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [02:11<00:00, 131.35s/it, loss=19]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(6.8987, grad_fn=<NllLossBackward0>) tensor(11.3557, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [02:04<00:00, 124.16s/it, loss=18.3]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(6.3236, grad_fn=<NllLossBackward0>) tensor(11.3238, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1/1 [01:59<00:00, 119.10s/it, loss=17.6]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(5.9397, grad_fn=<NllLossBackward0>) tensor(11.2387, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [02:12<00:00, 132.64s/it, loss=17.2]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(5.5605, grad_fn=<NllLossBackward0>) tensor(11.1801, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1/1 [02:48<00:00, 168.98s/it, loss=16.7]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(5.1987, grad_fn=<NllLossBackward0>) tensor(11.1533, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1/1 [02:05<00:00, 125.18s/it, loss=16.4]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(4.8477, grad_fn=<NllLossBackward0>) tensor(11.0438, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1/1 [01:45<00:00, 105.74s/it, loss=15.9]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(4.5807, grad_fn=<NllLossBackward0>) tensor(11.0147, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1/1 [02:19<00:00, 139.08s/it, loss=15.6]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(4.3275, grad_fn=<NllLossBackward0>) tensor(10.9365, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [02:03<00:00, 123.94s/it, loss=15.3]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(4.1134, grad_fn=<NllLossBackward0>) tensor(11.0169, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1/1 [02:17<00:00, 137.83s/it, loss=15.1]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.8899, grad_fn=<NllLossBackward0>) tensor(10.9574, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1/1 [02:45<00:00, 165.31s/it, loss=14.8]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.6850, grad_fn=<NllLossBackward0>) tensor(10.8558, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1/1 [02:08<00:00, 128.93s/it, loss=14.5]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.5318, grad_fn=<NllLossBackward0>) tensor(10.9320, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1/1 [02:04<00:00, 124.18s/it, loss=14.5]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.4499, grad_fn=<NllLossBackward0>) tensor(10.9252, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1/1 [02:01<00:00, 121.36s/it, loss=14.4]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.3459, grad_fn=<NllLossBackward0>) tensor(11.0252, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1/1 [02:23<00:00, 143.34s/it, loss=14.4]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.2740, grad_fn=<NllLossBackward0>) tensor(10.9399, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 1/1 [02:58<00:00, 178.81s/it, loss=14.2]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.2406, grad_fn=<NllLossBackward0>) tensor(10.9488, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 1/1 [02:32<00:00, 152.88s/it, loss=14.2]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.1527, grad_fn=<NllLossBackward0>) tensor(10.9427, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 1/1 [02:16<00:00, 136.06s/it, loss=14.1]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.1560, grad_fn=<NllLossBackward0>) tensor(10.9131, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1/1 [02:06<00:00, 126.11s/it, loss=14.1]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.1316, grad_fn=<NllLossBackward0>) tensor(10.9292, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 1/1 [02:57<00:00, 177.76s/it, loss=14.1]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.1251, grad_fn=<NllLossBackward0>) tensor(10.9089, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 1/1 [02:16<00:00, 136.54s/it, loss=14]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0652, grad_fn=<NllLossBackward0>) tensor(10.9100, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 1/1 [02:06<00:00, 126.33s/it, loss=14]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0368, grad_fn=<NllLossBackward0>) tensor(10.9073, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 1/1 [01:38<00:00, 98.22s/it, loss=13.9]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0146, grad_fn=<NllLossBackward0>) tensor(10.8928, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 1/1 [01:50<00:00, 110.25s/it, loss=13.9]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0214, grad_fn=<NllLossBackward0>) tensor(10.9012, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 1/1 [02:29<00:00, 149.22s/it, loss=13.9]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0015, grad_fn=<NllLossBackward0>) tensor(10.8813, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 1/1 [02:52<00:00, 172.23s/it, loss=13.9]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0193, grad_fn=<NllLossBackward0>) tensor(10.8859, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 1/1 [02:20<00:00, 140.95s/it, loss=13.9]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0348, grad_fn=<NllLossBackward0>) tensor(10.8798, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 1/1 [02:21<00:00, 141.93s/it, loss=13.9]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_differences.shape =  torch.Size([2, 1, 128, 50257]) torch.Size([1, 1, 2, 128, 50257]) torch.Size([2, 128, 1, 1, 50257]) torch.Size([2, 128, 2, 128, 50257])\n",
      "torch.Size([2, 128, 2, 128])\n",
      "tensor(3.0162, grad_fn=<NllLossBackward0>) tensor(10.8406, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 1/1 [02:10<00:00, 130.32s/it, loss=13.9]\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning using custom loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "lr = 1e-4\n",
    "num_epochs = 30\n",
    "\n",
    "optimizer = AdamW([param for param in model.parameters() if param.requires_grad], lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = custom_loss(outputs, batch, encoding_dict, lora_layers)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory to save the model if it doesn't exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "# save model checkpoint to models directory using current timestamp and date\n",
    "torch.save(model.state_dict(), f\"models/{time.strftime('%Y%m%d-%H%M%S')}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load latest model checkpoint among all the saved models\n",
    "latest_model = torch.load(max(glob.glob('models/*.pth'), key=os.path.getctime))\n",
    "# load the model with the latest checkpoint\n",
    "model.load_state_dict(latest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions for the test dataset\n",
    "generated_captions_custom_model = []\n",
    "# Iterate over the dataset and generate captions\n",
    "for data in dataset:\n",
    "    image = data['image']\n",
    "    generated_logits, generated_predicted_ids, caption = generate_caption_with_logits(image)\n",
    "    generated_captions_custom_model.append(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode compressed dictionary word using manual huffman encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace compressed_dict words occurring in the generated_captions_custom_model with their corresponding huffman encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare encoded generated_captions_custom_model + huffman encoding dictionary information with the original generated_captions to calculate compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a green fence next to a street next to a fence  WAIT a green fence next to a street next to a fence \n",
      "man crossing the street with a street light  WAIT man crossing the street with a street light \n"
     ]
    }
   ],
   "source": [
    "# print generated_captions and generated_captions_custom_model elementwise to compare the results\n",
    "for i in range(len(generated_captions)):\n",
    "    print (generated_captions[i], \"WAIT\", generated_captions_custom_model[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
